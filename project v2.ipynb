{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGG*3130 Final Project Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with required initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Word breakdowns\n",
    "import en_core_web_sm\n",
    "import tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from bs4 import BeautifulSoup # Not used, may be used in future\n",
    "from transformers import pipeline # Question answering pipeline\n",
    "import wikipediaapi\n",
    "# Found on SO, may be obsolete given I fixed other issues with packages\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function from Stack Overflow for extracting proper nouns\n",
    "# Credit goes to T. Jeanneau here: https://stackoverflow.com/questions/63450423/how-to-find-proper-noun-using-spacy-nlp\n",
    "# Function utilizes spaCy's proper noun tags\n",
    "def extract_proper_nouns(doc):\n",
    "        pos = [tok.i for tok in doc if tok.pos_ == \"PROPN\"]\n",
    "        consecutives = []\n",
    "        current = []\n",
    "        for elt in pos:\n",
    "            if len(current) == 0:\n",
    "                current.append(elt)\n",
    "            else:\n",
    "                if current[-1] == elt - 1:\n",
    "                    current.append(elt)\n",
    "                else:\n",
    "                    consecutives.append(current)\n",
    "                    current = [elt]\n",
    "        if len(current) != 0:\n",
    "            consecutives.append(current)\n",
    "        return [doc[consecutive[0]:consecutive[-1]+1] for consecutive in consecutives]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our class, and define some basic strings to hold our results. We also assign the value of the question, which should be passed to the class object on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class someone_make_an_acronym:\n",
    "    FINAL_TITLE = \"\" # Title that should give a correct Wikipedia article\n",
    "    WIKI_ARTICLE = \"\" # Full Wiki article text\n",
    "    QUESTION = \"\" # Question to be asked\n",
    "    result = \"\" # Data structure that holds the result from the transformer\n",
    "    def __init__(self, QUESTION):\n",
    "        self.QUESTION = QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Deconstruction\n",
    "Next, we need to deconstruct the question. Ultimately, we just need the subject the sentence is talking about. Linguistics can be very complicated. Our initial deconstruction attempt is to retrieve an 'entity', using spaCy. If this fails, a helper function is used, which detects concurrent proper nouns and groups them, like _George Washington_ or _Single-Nucleotide Polymorphism_.\n",
    "\n",
    "If this fails, an attempt is made to\n",
    "extract one of the following:\n",
    " - Compound noun\n",
    " - Nominal subject\n",
    " - Direct object\n",
    " - Object of a preposition\n",
    " \n",
    "For verbosity's sake, the specific linguistics of these four are ignored. How a question is constructed influences what parts of speech are correct for data extraction. So, like any sophisticated software, we test everything. \n",
    "\n",
    "The first part of speech that exists is considered the title for the Wikipedia article. There are obvious flaws with this method, but it usually returns correctly. \n",
    "\n",
    "Note: Keep in mind that the program flow only falls here if there are _no_ proper nouns, which makes question answering more ambiguous to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class someone_make_an_acronym(someone_make_an_acronym):\n",
    "    def BreakSentence(self):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nlp = en_core_web_sm.load()\n",
    "        doc=nlp(self.QUESTION)\n",
    "        # These are backups for not proper nouns\n",
    "        TITLE = \"\"\n",
    "        TITLE2 = \"\"\n",
    "        TITLE3 = \"\"\n",
    "        TITLE4 = \"\"\n",
    "        for ent in doc.ents:\n",
    "            self.FINAL_TITLE = ent.text\n",
    "            print(\"Entity detected, using: \" + self.FINAL_TITLE)\n",
    "            return\n",
    "        \n",
    "        # If we can't detect an entity, detect proper nouns instead\n",
    "        TEST_LIST = extract_proper_nouns(doc)\n",
    "        for item in TEST_LIST:\n",
    "            print(item.text + \"\\n\")\n",
    "        if(len(TEST_LIST)): # If proper noun extraction worked, we're good\n",
    "            self.FINAL_TITLE = str(TEST_LIST[0])\n",
    "            print(\"Proper noun(s) detected, using:\" + self.FINAL_TITLE)\n",
    "        else: # Otherwise, try to get ANY subject in sentence\n",
    "            print(\"No proper noun(s) deteced, attempting to extract subject.\")\n",
    "            # Backup for sentence without proper nouns\n",
    "            # Try everything remotely close to a noun or subject of sentence\n",
    "            sub_toks = [tok for tok in doc if (tok.dep_ == \"compound\") ]\n",
    "            sub_toks2 = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "            sub_toks3 = [tok for tok in doc if (tok.dep_ == \"dobj\") ]\n",
    "            sub_toks4 = [tok for tok in doc if (tok.dep_ == \"pobj\") ]\n",
    "            for value in sub_toks:\n",
    "                TITLE = str(value).replace(\" \", \"_\")\n",
    "                print(\"Noun compound detected...\")\n",
    "            for value in sub_toks2:\n",
    "                TITLE2 = str(value).replace(\" \", \"_\")\n",
    "                print(\"Nominal subject detected...\")\n",
    "            for value in sub_toks3:\n",
    "                TITLE3 = str(value).replace(\" \", \"_\")\n",
    "                print(\"Direct object detected...\")            \n",
    "            for value in sub_toks4:\n",
    "                TITLE4 = str(value).replace(\" \", \"_\")\n",
    "                print(\"Object of a preposition detected...\")\n",
    "            if len(TITLE):\n",
    "                self.FINAL_TITLE = str(TITLE)\n",
    "            elif len(TITLE2):\n",
    "                self.FINAL_TITLE = str(TITLE2)\n",
    "            elif len(TITLE3):\n",
    "                self.FINAL_TITLE = str(TITLE3)\n",
    "            elif len(TITLE4):\n",
    "                self.FINAL_TITLE = str(TITLE4)\n",
    "\n",
    "        if(len(self.FINAL_TITLE)):\n",
    "            print(self.FINAL_TITLE, len(self.FINAL_TITLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Wikipedia\n",
    "\n",
    "The original iteration of this can be seen at the end of the Notebook. It used Beautiful Soup to directly scrape a Wikipedia website link. This was clunky and proved difficult to debug. It also introduced noise in the form of extraneous text at the end of each page. It was decided to use a direct Wikipedia API. \n",
    "\n",
    "The Wikipedia object is created, and then the title extracted from the given question is fed to the object. If it is a valid Wikipedia article, the text (or just summary, if desired) is returned. This provides very concise context for the question answering pipeline. Both summary or full text could be considered as context, depending on the question scope. For example, if a general question is asked, the summary may be sufficient. This could be considered for large question sets. More specific questions may require the entire text for a correct answer.\n",
    "\n",
    "We assign the Wikipedia article to `WIKI_ARTICLE` and use it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class someone_make_an_acronym(someone_make_an_acronym):\n",
    "    def ScrapeAndSanitizeWiki(self):\n",
    "        wiki_object = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        )\n",
    "        \n",
    "        wiki_article = wiki_object.page(self.FINAL_TITLE)\n",
    "        print(wiki_article)\n",
    "        self.WIKI_ARTICLE  = wiki_article.summary #text or summary       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the Question\n",
    "Using Huggingface pipeline for question answering allows incredibly simple use. We create the pipeline object for question answering, pass the Wikipedia article as context, the question asked as the question, and assign `topk` to 3 (as this is how many answers we would like to see). Then, `self.result` is assigned to be the result of the processed question. It is a few nested arrays, so it must be parsed in a specific way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class someone_make_an_acronym(someone_make_an_acronym):\n",
    "    def AnswerQuestion(self):\n",
    "        # Answer the question\n",
    "        # Determine what parts are taking the longest (assumption: result = nlp)\n",
    "        nlp = pipeline(\"question-answering\")\n",
    "        context = self.WIKI_ARTICLE\n",
    "        self.result = nlp(question=self.QUESTION, context=context, topk = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results\n",
    "The results are then printed. We asked for 3 answers, and we print 3 answers, as well as the confidence of a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class someone_make_an_acronym(someone_make_an_acronym):\n",
    "    def PrintQnA(self):\n",
    "        print(\"\\n\" + self.QUESTION + \"\\n\")\n",
    "        for i in range(0,3): # Increase for more sets of answers - this has no computation delay as they are already stored.\n",
    "            print(str(self.result[i]['answer']), (25 - len(str(self.result[i]['answer'])))*\" \", str(self.result[i]['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calls\n",
    "We call each function, making sure to include a question! The more straightforward the question, the more correct the answer.\n",
    "\n",
    "Note: Try to ask a few questions! You might be surprised how correct they are. Depending on context length and computer speed, it may take up to 30 seconds to receive an answer. If there is an error (or the sentence is not parsed properly) it will error fairly quickly and not delay.\n",
    "\n",
    "Here are some good questions you could ask:\n",
    " - What is MSG used for?\n",
    " - What is the Earth made of?\n",
    " - Where is Saskatchewan?\n",
    " - What did Steve Jobs do before Apple?\n",
    " - What is the Meaning of Life?\n",
    " - What are _______ made from? (try chimneys, pencils, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Where is Saskatchewan?\n",
      "\n",
      "Western Canada             0.938798725605011\n",
      "in Western Canada          0.022149959579110146\n",
      "northern boreal half is mostly forested and sparsely populated  0.010969744995236397\n"
     ]
    }
   ],
   "source": [
    "test3 = someone_make_an_acronym(\"Where is Saskatchewan?\")\n",
    "test3.BreakSentence()\n",
    "test3.ScrapeAndSanitizeWiki()\n",
    "test3.AnswerQuestion()\n",
    "test3.PrintQnA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the Wikipedia article, for reference.\n",
    "print(test3.WIKI_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Outdated Wikipedia scraping using Beautiful Soup\n",
    "    html = urlopen(\"https://en.wikipedia.org/wiki/\" + self.FINAL_TITLE)\n",
    "    parsed_html = BeautifulSoup(html)\n",
    "    paragraphs = parsed_html.select(\"p\")\n",
    "    print(paragraphs)\n",
    "    clean_html = paragraphs.body.findAll(text=True)\n",
    "    no_newline_html = list(filter((\"\\n\").__ne__, clean_html))\n",
    "    self.WIKI_ARTICLE = TreebankWordDetokenizer().detokenize(clean_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
